{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c6d0b1",
   "metadata": {},
   "source": [
    "A ideia deste notebook é coletar a base final após toda a padronização e enriquecimento para iniciar o processo de preparação dos dados. Essa preparação envolve criar pares de caso-diagnóstico que serão tokenizados e utilizados para o treinamento do modelo LLM. Além disso, também pretendo coletar estudos clínicos e pesquisas sobre as doenças de modo a utilizar na contextualização do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e57a406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do pytorch:  2.4.1+cu121\n",
      "CUDA disponível: True\n",
      "Versão do CUDA compatível com PyTorch: 12.1\n",
      "Dispositivo CUDA: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#Testando a configuração do pytorch para garantir o uso da GPU (Nvidia RTX 3050) no treinamento do modelo\n",
    "import torch\n",
    "print(\"Versão do pytorch: \", torch.__version__)\n",
    "print(\"CUDA disponível:\", torch.cuda.is_available())\n",
    "print(\"Versão do CUDA compatível com PyTorch:\", torch.version.cuda)\n",
    "print(\"Dispositivo CUDA:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Nenhum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f93d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importando as libs necessárias\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc2f72",
   "metadata": {},
   "source": [
    "# Processando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80ffe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando dataset unido e padronizado\n",
    "merged_dataset = pd.read_csv(\"./datasets/merged_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a4c17bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diseases</th>\n",
       "      <th>abdomen acute</th>\n",
       "      <th>abdomen distended</th>\n",
       "      <th>abdominal bloating</th>\n",
       "      <th>abdominal colic</th>\n",
       "      <th>abdominal pain</th>\n",
       "      <th>abdominal tenderness</th>\n",
       "      <th>abnormal appearing skin</th>\n",
       "      <th>abnormal appearing tongue</th>\n",
       "      <th>abnormal breathing sounds</th>\n",
       "      <th>...</th>\n",
       "      <th>wrist pain</th>\n",
       "      <th>wrist stiffness or tightness</th>\n",
       "      <th>wrist swelling</th>\n",
       "      <th>wrist weakness</th>\n",
       "      <th>yellow color</th>\n",
       "      <th>yellow crust ooze</th>\n",
       "      <th>yellow sputum</th>\n",
       "      <th>yellowing of eyes</th>\n",
       "      <th>diseases_description</th>\n",
       "      <th>disease_risk_factors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Panic Disorder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A type of anxiety disorder characterized by un...</td>\n",
       "      <td>Symptoms of panic disorder often start in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Panic Disorder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A type of anxiety disorder characterized by un...</td>\n",
       "      <td>Symptoms of panic disorder often start in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Panic Disorder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A type of anxiety disorder characterized by un...</td>\n",
       "      <td>Symptoms of panic disorder often start in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Panic Disorder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A type of anxiety disorder characterized by un...</td>\n",
       "      <td>Symptoms of panic disorder often start in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Panic Disorder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A type of anxiety disorder characterized by un...</td>\n",
       "      <td>Symptoms of panic disorder often start in the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 839 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         diseases  abdomen acute  abdomen distended  abdominal bloating  \\\n",
       "0  Panic Disorder              0                  0                   0   \n",
       "1  Panic Disorder              0                  0                   0   \n",
       "2  Panic Disorder              0                  0                   0   \n",
       "3  Panic Disorder              0                  0                   0   \n",
       "4  Panic Disorder              0                  0                   0   \n",
       "\n",
       "   abdominal colic  abdominal pain  abdominal tenderness  \\\n",
       "0                0               0                     0   \n",
       "1                0               0                     0   \n",
       "2                0               0                     0   \n",
       "3                0               0                     0   \n",
       "4                0               0                     0   \n",
       "\n",
       "   abnormal appearing skin  abnormal appearing tongue  \\\n",
       "0                        0                          0   \n",
       "1                        0                          0   \n",
       "2                        0                          0   \n",
       "3                        0                          0   \n",
       "4                        0                          0   \n",
       "\n",
       "   abnormal breathing sounds  ...  wrist pain  wrist stiffness or tightness  \\\n",
       "0                          0  ...           0                             0   \n",
       "1                          0  ...           0                             0   \n",
       "2                          0  ...           0                             0   \n",
       "3                          0  ...           0                             0   \n",
       "4                          0  ...           0                             0   \n",
       "\n",
       "   wrist swelling  wrist weakness  yellow color  yellow crust ooze  \\\n",
       "0               0               0             0                  0   \n",
       "1               0               0             0                  0   \n",
       "2               0               0             0                  0   \n",
       "3               0               0             0                  0   \n",
       "4               0               0             0                  0   \n",
       "\n",
       "   yellow sputum  yellowing of eyes  \\\n",
       "0              0                  0   \n",
       "1              0                  0   \n",
       "2              0                  0   \n",
       "3              0                  0   \n",
       "4              0                  0   \n",
       "\n",
       "                                diseases_description  \\\n",
       "0  A type of anxiety disorder characterized by un...   \n",
       "1  A type of anxiety disorder characterized by un...   \n",
       "2  A type of anxiety disorder characterized by un...   \n",
       "3  A type of anxiety disorder characterized by un...   \n",
       "4  A type of anxiety disorder characterized by un...   \n",
       "\n",
       "                                disease_risk_factors  \n",
       "0  Symptoms of panic disorder often start in the ...  \n",
       "1  Symptoms of panic disorder often start in the ...  \n",
       "2  Symptoms of panic disorder often start in the ...  \n",
       "3  Symptoms of panic disorder often start in the ...  \n",
       "4  Symptoms of panic disorder often start in the ...  \n",
       "\n",
       "[5 rows x 839 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualizando o dataset\n",
    "merged_dataset.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "merged_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a1d4f",
   "metadata": {},
   "source": [
    "> Com os dados já importados devidamente, é preciso processar o dataset para o processo de tokenização. A base será estruturada em pares de caso-diagnóstico, no caso serão descritos os sintomas daquela instância marcados como 1 e no diagnóstico estará a doença com sua devida descrição e fatores de rrisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f2e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para geração dos pares na base\n",
    "COLUNAS = merged_dataset.columns\n",
    "def gerar_pares(row):\n",
    "    # Geração do input com base nos sintomas marcados como 1\n",
    "    sintomas = [col.replace(\"_\", \" \") for col in COLUNAS if row[col] == 1]\n",
    "    input_text = f\"The pacient presents the following symptoms: {', '.join(sintomas)}.\"\n",
    "\n",
    "    # Geração do output com diagnóstico + descrição + fatores de risco\n",
    "    output_text = f'''\n",
    "        Diagnosis: {row['diseases']}.\\n\n",
    "        Description: {row['diseases_description']}.\\n\n",
    "        Risk factors: {row['disease_risk_factors']}.\n",
    "    '''\n",
    "    \n",
    "    return {\"input\": input_text, \"output\": output_text} #retorno do par gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1336ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agora é só ler a base e aplicar a geração dos pares\n",
    "caso_diagnostico = merged_dataset.apply(gerar_pares, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326ef835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'The pacient presents the following symptoms: anxiety and nervousness, breathing fast, chest tightness, depressive or psychotic symptoms, irregular heartbeat, palpitations, shortness of breath.',\n",
       " 'output': '\\n        Diagnosis: Panic Disorder.\\n\\n        Description: A type of anxiety disorder characterized by unexpected panic attacks that last minutes or, rarely, hours. Panic attacks begin with intense apprehension, fear or terror and, often, a feeling of impending doom. Symptoms experienced during a panic attack include dyspnea or sensations of being smothered; dizziness, loss of balance or faintness; choking sensations; palpitations or accelerated heart rate; shakiness; sweating; nausea or other form of abdominal distress; depersonalization or derealization; paresthesias; hot flashes or chills; chest discomfort or pain; fear of dying and fear of not being in control of oneself or going crazy. Agoraphobia may also develop. Similar to other anxiety disorders, it may be inherited as an autosomal dominant trait..\\n\\n        Risk factors: Symptoms of panic disorder often start in the late teens or early adulthood and affect more women than men. Factors that may increase the risk of developing panic attacks or panic disorder include: Family history of panic attacks or panic disorderMajor life stress, such as the death or serious illness of a loved oneA traumatic event, such as sexual assault or a serious accidentMajor changes in your life, such as a divorce or the addition of a babySmoking or excessive caffeine intakeHistory of childhood physical or sexual abuse.\\n    '}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemplo de par gerado a partir da base\n",
    "caso_diagnostico[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff13124",
   "metadata": {},
   "source": [
    "# Selecionando o modelo de LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03148064",
   "metadata": {},
   "source": [
    "Para o modelo eu decidi utilizar o BioGPT que  é um modelo de linguagem desenvolvido pela Microsoft Research especificamente para tarefas biomédicas. Ele segue a arquitetura dos Transformers (GPT-style), mas foi treinado exclusivamente com textos biomédicos, como artigos do PubMed, abstracts científicos e literatura médica especializada. \n",
    "\n",
    "**Referência:** https://huggingface.co/microsoft/biogpt\n",
    "\n",
    "**Artigo de Referência:**\n",
    "\n",
    "LUO, Renqian et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, [S.l.], v. 23, n. 6, set. 2022. Disponível em: https://doi.org/10.1093/bib/bbac409."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f5e91fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "#código exemplo para a utilização do modelo BioGPT\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM\n",
    "model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\") #instanciando o modelo\n",
    "\n",
    "#movendo o modelo para a gpu do sistema (Nvidia RTX 3050)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\") #instanciando o tokenizer\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer) #criando o gerador de texto\n",
    "set_seed(42) #configurando semente aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b77630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\transformers\\models\\biogpt\\modeling_biogpt.py:330: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Influenza is a worldwide cause of respiratory disease that causes morbidity and mortality worldwide.'},\n",
       " {'generated_text': 'Influenza is an infection which continues to cause considerable global morbidity over the course of an infectious season.'},\n",
       " {'generated_text': 'Influenza is a viral pathogen that poses a threat to public health.'},\n",
       " {'generated_text': 'Influenza is a highly infectious respiratory disease that is still a major cause of morbidity and mortality in the'},\n",
       " {'generated_text': 'Influenza is an acute viral infection, while rotavirus is an infectious agent that commonly causes gastroenteritis.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemplo de como gerar o texto com o modelo\n",
    "generator(\"Influenza is\", max_length=20, num_return_sequences=5, do_sample=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25b90b",
   "metadata": {},
   "source": [
    "# Tokenizando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e31d81",
   "metadata": {},
   "source": [
    "Modelos de linguagem não entendem texto diretamente. Eles precisam do texto transformado em tokens numéricos. A tokenização converte os inputs e outputs em listas de números compreensíveis para o modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104ccecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde1dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output'],\n",
      "    num_rows: 263609\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#cria dataset Hugging Face com os pares\n",
    "dataset = Dataset.from_list(caso_diagnostico)\n",
    "print(dataset) #dataset preparado\n",
    "\n",
    "#separando treino/validação\n",
    "dataset = dataset.train_test_split(test_size=0.15)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eed40054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 224067/224067 [05:48<00:00, 643.06 examples/s]\n",
      "Map: 100%|██████████| 39542/39542 [01:01<00:00, 638.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 224067\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input', 'output', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 39542\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#como o BioGPT é causal LM (autogerativo), vamos concatenar input + output e treinar o modelo para prever\n",
    "def tokenize_function(example): #função para tokenizar os dados antes do treinamento\n",
    "    prompt = example[\"input\"] + \"\\n\" + example[\"output\"]\n",
    "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "#dados tokenizados\n",
    "tokenized_train = train_dataset.map(tokenize_function)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function)\n",
    "\n",
    "print(tokenized_train) \n",
    "print(tokenized_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919073ec",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "064e1fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_31884\\3076494084.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "#configurando os dados do treinamento\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biogpt-finetuned\",\n",
    "    evaluation_strategy=\"epoch\", #estratégia de treinamento por épocas\n",
    "    learning_rate=5e-5, #taxa de aprendizagem do modelo\n",
    "    per_device_train_batch_size=3, #tamanho do batch de treino\n",
    "    per_device_eval_batch_size=3, #tamanho do batch de validacao\n",
    "    num_train_epochs=3, #numero de epocas de treino\n",
    "    weight_decay=0.01, #taxa de decaimento dos pesos\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "#como é causal LM, usamos esse collato, dado uma lista de exemplos, retorna um batch pronto para o modelo\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False #serve para criar tensores compatíveis para o modelo (inputs, labels, masks, etc.) e\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c658fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc42a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/84027 [03:17<2305:54:56, 98.80s/it]\n",
      "  0%|          | 0/224067 [00:00<?, ?it/s]c:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "                                                       \n",
      "  0%|          | 10/224067 [01:26<543:58:48,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4945, 'grad_norm': 5.845268726348877, 'learning_rate': 4.9997768524593095e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 20/224067 [02:55<550:34:28,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5691, 'grad_norm': 5.299700736999512, 'learning_rate': 4.999553704918618e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 30/224067 [04:26<559:05:14,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2282, 'grad_norm': 4.432202339172363, 'learning_rate': 4.999330557377927e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 40/224067 [05:56<550:39:33,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1662, 'grad_norm': 6.782186508178711, 'learning_rate': 4.9991074098372366e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 50/224067 [07:25<549:25:10,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9061, 'grad_norm': 5.263533115386963, 'learning_rate': 4.998884262296545e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 60/224067 [08:54<548:06:27,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0468, 'grad_norm': 5.263553619384766, 'learning_rate': 4.9986611147558544e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 70/224067 [10:23<547:42:29,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9587, 'grad_norm': 3.8929741382598877, 'learning_rate': 4.9984379672151636e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 80/224067 [11:52<549:04:50,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8288, 'grad_norm': 5.149533271789551, 'learning_rate': 4.998214819674473e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 90/224067 [13:23<561:41:32,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7261, 'grad_norm': 7.742895603179932, 'learning_rate': 4.9979916721337814e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  0%|          | 100/224067 [14:55<569:59:08,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7261, 'grad_norm': 5.822592735290527, 'learning_rate': 4.997768524593091e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  0%|          | 110/224067 [16:26<568:44:27,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9363, 'grad_norm': 5.027202606201172, 'learning_rate': 4.9975453770524e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  0%|          | 120/224067 [17:57<562:30:01,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0735, 'grad_norm': 6.088617324829102, 'learning_rate': 4.997322229511709e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  0%|          | 130/224067 [19:28<561:11:22,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3603, 'grad_norm': 4.004209995269775, 'learning_rate': 4.997099081971018e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 135/224067 [20:12<557:51:06,  8.97s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#agora que tudo já foi preparado, vamos realizar o treinamento do modelo\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\transformers\\trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\accelerate\\accelerator.py:2242\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2242\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\medical_llm\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#agora que tudo já foi preparado, vamos realizar o treinamento do modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvando o modelo treinado\n",
    "trainer.save_model(\"biogpt-finetuned-symptom-diagnosis\")\n",
    "tokenizer.save_pretrained(\"biogpt-finetuned-symptom-diagnosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#teste de inferência do modelo com fine-tuning\n",
    "generator = pipeline('text-generation', model=\"biogpt-finetuned-symptom-diagnosis\", tokenizer=tokenizer)\n",
    "generator(\"The pacient presents the following symptoms: fever, cough, fatigue.\", max_length=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

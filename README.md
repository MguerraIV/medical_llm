# ğŸ§  Specializing LLMs for Medical Diagnosis: A Fine-Tuning-Based Approach

Este repositÃ³rio contÃ©m os artefatos desenvolvidos para a pesquisa de TCC intitulada **â€œSpecializing LLMs for Medical Diagnosis: A Fine-Tuning-Based Approachâ€**, conduzida por **MÃ¡rio Guerra** como parte da graduaÃ§Ã£o em Engenharia da ComputaÃ§Ã£o pela UPE. A proposta central Ã© investigar o impacto da sobreposiÃ§Ã£o de datasets mÃ©dicos heterogÃªneos na performance de **Language Models** aplicados ao diagnÃ³stico clÃ­nico.

---

## ğŸ“Œ HipÃ³tese de Pesquisa

> *â€œA sobreposiÃ§Ã£o de datasets heterogÃªneos, por meio da sÃ­ntese de caracterÃ­sticas mÃ©dicas comuns e representativas, reduz o viÃ©s e aumenta a precisÃ£o de LLMs no diagnÃ³stico mÃ©dico.â€*

---

## ğŸ—‚ Estrutura do RepositÃ³rio

```
ğŸ“ medical_llm/
â”‚
â”œâ”€â”€ ğŸ“’ 1_data_preprocessing.ipynb     # PrÃ©-processamento, unificaÃ§Ã£o e enriquecimento dos dados
â”œâ”€â”€ ğŸ“’ 2_data_completion_biogpt.ipynb # GeraÃ§Ã£o de dados sintÃ©ticos com BioGPT
â”œâ”€â”€ ğŸ“’ 3_model_training.ipynb         # PreparaÃ§Ã£o dos pares texto-alvo, tokenizaÃ§Ã£o e fine-tuning
â”œâ”€â”€ ğŸ“‚ data/                          # Datasets originais e processados
â”œâ”€â”€ ğŸ“‚ utils/                         # Scripts auxiliares (tokenizaÃ§Ã£o, limpeza, etc.)
â””â”€â”€ README.md                        # Este arquivo
```

---

## ğŸ“š Bases de Dados Utilizadas

A pesquisa integra e sobrepÃµe datasets distintos do tipo **disease-symptom**, com sintomas codificados como vetores binÃ¡rios (`0` ou `1`) e uma coluna-alvo com o diagnÃ³stico correspondente:

- **OpenML Medical Datasets**  
- **SymCAT** â€“ DiagnÃ³sticos mapeados a partir de sintomas com base em prevalÃªncia.
- **Disease Symptom Knowledge Database** â€“ RelaÃ§Ãµes estruturadas entre doenÃ§as e sintomas.
- *(Suporte secundÃ¡rio: MIMIC-III, eICU, ClinicalTrials â€“ referenciados para futura extensÃ£o)*

---

## ğŸ§ª Metodologia

### âœ… 1. Coleta, Limpeza e UnificaÃ§Ã£o
- NormalizaÃ§Ã£o de nomes de sintomas e doenÃ§as via **SciSpacy** + **UMLS**.
- Engenharia de features para harmonizaÃ§Ã£o inter-base.
- Enriquecimento semÃ¢ntico com descriÃ§Ãµes e fatores de risco (via web scraping da [Mayo Clinic](https://www.mayoclinic.org/)).

### âœ… 2. Preenchimento com BioGPT
- DoenÃ§as sem descriÃ§Ã£o foram completadas via geraÃ§Ã£o sintÃ©tica com **BioGPT**.
- A geraÃ§Ã£o abrangeu:
  - DefiniÃ§Ãµes clÃ­nicas
  - Fatores de risco
  - Contexto epidemiolÃ³gico

### âœ… 3. PreparaÃ§Ã£o dos Dados para Fine-Tuning
- ConstruÃ§Ã£o de pares `input-text` â†’ `diagnÃ³stico esperado`.
- Exemplo:
  ```
  Input: Patient presents with fatigue, low-grade fever, muscle pain and dry cough.
  Output: Influenza
  ```
- TokenizaÃ§Ã£o com tokenizer do **BioGPT**.

### âœ… 4. Fine-Tuning do BioGPT
- Treinamento local com instÃ¢ncia do modelo usando PyTorch + HuggingFace Transformers.
- DivisÃ£o de dados:
  - 70% treino
  - 15% validaÃ§Ã£o
  - 15% teste

---

## ğŸ§  Arquitetura do Modelo

- **Modelo-base**: [`BioGPT`](https://huggingface.co/microsoft/BioGPT)
- **Paradigma de ajuste**: Fine-tuning supervisionado
- **HiperparÃ¢metros principais**:
  - Learning rate: `5e-5`
  - Batch size: `16`
  - Epochs: `5`

---

## ğŸ“ AvaliaÃ§Ã£o de Desempenho

- **MÃ©tricas aplicadas**:
  - Accuracy
  - F1-Score
  - Recall
  - AUROC
  - ROUGE (em respostas explicativas)

- **EstratÃ©gia de Controle**:
  - ComparaÃ§Ã£o com modelo base (BioGPT sem fine-tuning)
  - Testes com casos reais e simulados
  - ValidaÃ§Ã£o qualitativa por especialistas mÃ©dicos (etapa futura)

---

## ğŸ“ˆ Resultados Esperados

A sobreposiÃ§Ã£o de mÃºltiplas fontes clÃ­nicas resulta em:
- Maior cobertura de doenÃ§as raras
- ReduÃ§Ã£o de viÃ©s de prevalÃªncia
- GeraÃ§Ã£o de descriÃ§Ãµes clinicamente coerentes
- Melhora na acurÃ¡cia de prediÃ§Ã£o em sintomas diversos

---

## ğŸ”¬ ConclusÃ£o ProvisÃ³ria

A fusÃ£o de datasets mÃ©dicos complementares, aliada ao enriquecimento textual, potencializa a especializaÃ§Ã£o de LLMs para tarefas diagnÃ³sticas. A capacidade do modelo de generalizar para contextos clÃ­nicos heterogÃªneos aumenta consideravelmente com a presenÃ§a de exemplos variados e bem documentados.

---

## ğŸ›  Tecnologias e Ferramentas

- `Python`, `Pandas`, `NumPy`
- `SciSpacy`, `UMLS`, `requests`, `BeautifulSoup`
- `HuggingFace Transformers`, `BioGPT`
- `Jupyter Notebooks`

---

## ğŸ“ Como Executar

1. Clone o repositÃ³rio:
   ```bash
   git clone https://github.com/MguerraIV/medical_llm.git
   cd medical_llm
   ```

2. Crie um ambiente e instale as dependÃªncias:
   ```bash
   pip install -r requirements.txt
   ```

3. Execute os notebooks na ordem:
   - `1_data_preprocessing.ipynb`
   - `2_data_completion_biogpt.ipynb`
   - `3_model_training.ipynb`

---

## ğŸ§¾ ReferÃªncias BibliogrÃ¡ficas

- Lee, J. et al. (2020). *BioBERT: a pre-trained biomedical language representation model for biomedical text mining*. Bioinformatics.
- Chen, Q. et al. (2022). *BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining*.
- Mayo Clinic. [https://www.mayoclinic.org](https://www.mayoclinic.org)
- NEUMANN, Mark et al. ScispaCy: fast and robust models for biomedical natural language processing. In: Proceedings of the 18th BioNLP Workshop and Shared Task, Florence, Italy, 2019. Association for Computational Linguistics, p. 319â€“327. DisponÃ­vel em: https://aclanthology.org/W19-5034.
- BODENREIDER, Olivier. The Unified Medical Language System (UMLS): integrating biomedical terminology. Nucleic Acids Research, [S.l.], v. 32, n. Database issue, p. D267â€“D270, 2004. Oxford University Press. DisponÃ­vel em: https://doi.org/10.1093/nar/gkh061. 
---

## âœ‰ï¸ Contato

**MÃ¡rio Guerra**  
Universidade de Pernambuco â€“ UPE  
ğŸ“§ [msg4@poli.br](mailto:msg4@poli.br)
